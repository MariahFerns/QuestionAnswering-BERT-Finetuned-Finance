{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e8f187d33e8242bfbad8057a69d504f6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6c74a8ac6ec4b6bb5d558ac09c3cb2a","IPY_MODEL_a9ada3e398dd48b7a2153c19bde0b787","IPY_MODEL_5d2fd8542144457f82f268c66e052cee"],"layout":"IPY_MODEL_988c8a3ea1f64c1f893b2acde0410259"}},"d6c74a8ac6ec4b6bb5d558ac09c3cb2a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98b8d435ecd44c228ee41ff6eb5ed8f0","placeholder":"​","style":"IPY_MODEL_d46771552f384adfbd0be8c45ea22b7a","value":"Map: 100%"}},"a9ada3e398dd48b7a2153c19bde0b787":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b67f112e60449b7a7c3eecd0069d750","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61dd301067a144138c238dba5c1b7bb6","value":500}},"5d2fd8542144457f82f268c66e052cee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bf2fce401b4479d8f2bac1f2c33b92f","placeholder":"​","style":"IPY_MODEL_58ebd793d35844749d5b5156ca9734fb","value":" 500/500 [00:00&lt;00:00, 854.46 examples/s]"}},"988c8a3ea1f64c1f893b2acde0410259":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98b8d435ecd44c228ee41ff6eb5ed8f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d46771552f384adfbd0be8c45ea22b7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b67f112e60449b7a7c3eecd0069d750":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61dd301067a144138c238dba5c1b7bb6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7bf2fce401b4479d8f2bac1f2c33b92f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58ebd793d35844749d5b5156ca9734fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"129163357e194e82a0e106a7c4420da2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e7eb29faaa8f4c77af6822936474b6d4","IPY_MODEL_b0596fe8d336432788d0db93c7f4a751","IPY_MODEL_9438e5db0c1e4c28aaeae246b423dd6a"],"layout":"IPY_MODEL_edd4ce40c0684ac9905b5972649ca9a4"}},"e7eb29faaa8f4c77af6822936474b6d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec17a97c2d964619aa4730c9677bd7ea","placeholder":"​","style":"IPY_MODEL_a8fab1c701d54f198a6cf84262a22544","value":"Map: 100%"}},"b0596fe8d336432788d0db93c7f4a751":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ef3dae54dd6429db030c5750424d11b","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d1fe151114446f1af91d8bac22ec2c6","value":50}},"9438e5db0c1e4c28aaeae246b423dd6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fa464c0c04048d09577324aa986b4b3","placeholder":"​","style":"IPY_MODEL_c4e3d174794e4001a7d02b7abf22cddb","value":" 50/50 [00:00&lt;00:00, 513.14 examples/s]"}},"edd4ce40c0684ac9905b5972649ca9a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec17a97c2d964619aa4730c9677bd7ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8fab1c701d54f198a6cf84262a22544":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ef3dae54dd6429db030c5750424d11b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d1fe151114446f1af91d8bac22ec2c6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8fa464c0c04048d09577324aa986b4b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4e3d174794e4001a7d02b7abf22cddb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"544f0955c4744b278d0bf7b430d8eabc":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_fc277748015d45fd8fb80680cd8536b0","IPY_MODEL_06c132f8e2c044ebb8d5e9377b0e7cd9","IPY_MODEL_c0e4c799e681449ba0e450a70ab80ca6","IPY_MODEL_a44617fa2c2649569bac5940a5831f62","IPY_MODEL_679029a59eb24a38bf3b4ca7b7d9b804"],"layout":"IPY_MODEL_70ca975a3cb54ee5ad138a1acb1edcdf"}},"fc277748015d45fd8fb80680cd8536b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddc8f26e87604d9db747f8b0e349eb1a","placeholder":"​","style":"IPY_MODEL_7429a88209f04493906ccbbccc834b24","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"06c132f8e2c044ebb8d5e9377b0e7cd9":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_2f438ad27cf54d0db5568fb9538afa34","placeholder":"​","style":"IPY_MODEL_e7403a1b35074f91b7535801b4add88f","value":""}},"c0e4c799e681449ba0e450a70ab80ca6":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_98c29b2aa0fb44cc8921d3d5dce507f9","style":"IPY_MODEL_722d80e36f5443d9999e57c511ee55ff","value":true}},"a44617fa2c2649569bac5940a5831f62":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_833f188e766447a6aa3a464bf3db4b01","style":"IPY_MODEL_ce5ad6e2873e41c48207b8afae4ee6f5","tooltip":""}},"679029a59eb24a38bf3b4ca7b7d9b804":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aac84ab802104c4196c6357393895a17","placeholder":"​","style":"IPY_MODEL_03cc1cea622f4552859a88dc9bdbf207","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"70ca975a3cb54ee5ad138a1acb1edcdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"ddc8f26e87604d9db747f8b0e349eb1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7429a88209f04493906ccbbccc834b24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f438ad27cf54d0db5568fb9538afa34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7403a1b35074f91b7535801b4add88f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98c29b2aa0fb44cc8921d3d5dce507f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"722d80e36f5443d9999e57c511ee55ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"833f188e766447a6aa3a464bf3db4b01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce5ad6e2873e41c48207b8afae4ee6f5":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"aac84ab802104c4196c6357393895a17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03cc1cea622f4552859a88dc9bdbf207":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a018a6568bbb4d6cb3afb04d734d438c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00642b1cb6a24021829f1a2a840e407f","IPY_MODEL_97474b2d2c90402d9947c01e01eb5a28","IPY_MODEL_8084d22f0ebb4200954d028f1b1a060c"],"layout":"IPY_MODEL_d95771be753d46518d634ed67642ac67"}},"00642b1cb6a24021829f1a2a840e407f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4582cb10c1024dafa852d46d424c305d","placeholder":"​","style":"IPY_MODEL_60d983e2f2784bba95c9d7afa594117a","value":"Map: 100%"}},"97474b2d2c90402d9947c01e01eb5a28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3b789b657eb4fab804d1c655aedbe3e","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a99a165bfdad4aa298cbca24ddb766d0","value":50}},"8084d22f0ebb4200954d028f1b1a060c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22d02df4d11a4751ae3ae8976f0c7d92","placeholder":"​","style":"IPY_MODEL_7ef815767bd94672a9747b2b1437c7f3","value":" 50/50 [00:00&lt;00:00, 495.83 examples/s]"}},"d95771be753d46518d634ed67642ac67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4582cb10c1024dafa852d46d424c305d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60d983e2f2784bba95c9d7afa594117a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3b789b657eb4fab804d1c655aedbe3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a99a165bfdad4aa298cbca24ddb766d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"22d02df4d11a4751ae3ae8976f0c7d92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ef815767bd94672a9747b2b1437c7f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":53352,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":44767}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import BertTokenizer, BertForQuestionAnswering\nfrom transformers import AutoTokenizer, AutoModel\nfrom transformers import AutoModelForQuestionAnswering, TFAutoModelForQuestionAnswering\nfrom transformers import pipeline\nimport transformers\nimport torch\nimport os\nfrom tqdm import tqdm\nimport json\nimport re\nimport pandas as pd\nimport torch\nimport time\nimport tensorflow as tf","metadata":{"id":"ay2jnTZWXUq2","execution":{"iopub.status.busy":"2024-05-23T07:49:06.301496Z","iopub.execute_input":"2024-05-23T07:49:06.301882Z","iopub.status.idle":"2024-05-23T07:49:31.004295Z","shell.execute_reply.started":"2024-05-23T07:49:06.301852Z","shell.execute_reply":"2024-05-23T07:49:31.003022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup initial parameters\n\nsquad_v2 = False # indicates if impossible answers are allowed\nmodel_checkpoint = 'distilbert-base-uncased'\nbatch_size = 16","metadata":{"id":"V4tD0NQzlzJE","execution":{"iopub.status.busy":"2024-05-23T07:49:31.006461Z","iopub.execute_input":"2024-05-23T07:49:31.007252Z","iopub.status.idle":"2024-05-23T07:49:31.012935Z","shell.execute_reply.started":"2024-05-23T07:49:31.007186Z","shell.execute_reply":"2024-05-23T07:49:31.011530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset and metric for evaluation\n\nfrom datasets import load_dataset, load_metric\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\n# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"Bl0Qi83vXWHL","outputId":"af3d23f9-1ba9-4f7f-ecc2-7fd9807d272e","execution":{"iopub.status.busy":"2024-05-23T07:49:31.014880Z","iopub.execute_input":"2024-05-23T07:49:31.015378Z","iopub.status.idle":"2024-05-23T07:49:31.575172Z","shell.execute_reply.started":"2024-05-23T07:49:31.015335Z","shell.execute_reply":"2024-05-23T07:49:31.573891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the SQuAD dataset\n\n# use a smaller size for testing\ntrain_size = 300 #87599\nvalid_size = 50 #10570\n\nsplit = [\n    f\"train[:{train_size}]\",\n    f\"validation[:{valid_size}]\"\n]\n\n\ntrain, validation = load_dataset('squadv2' if squad_v2 else 'squad', split=split)\n\n","metadata":{"id":"hUpzOZziXY1H","execution":{"iopub.status.busy":"2024-05-23T07:49:31.578066Z","iopub.execute_input":"2024-05-23T07:49:31.578823Z","iopub.status.idle":"2024-05-23T07:49:40.896342Z","shell.execute_reply.started":"2024-05-23T07:49:31.578786Z","shell.execute_reply":"2024-05-23T07:49:40.895014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"id":"0QoBgKuL3iQf","outputId":"bd222c54-2a14-4205-eee1-2776bb6db523","execution":{"iopub.status.busy":"2024-05-23T07:49:40.898035Z","iopub.execute_input":"2024-05-23T07:49:40.898475Z","iopub.status.idle":"2024-05-23T07:49:40.909569Z","shell.execute_reply.started":"2024-05-23T07:49:40.898439Z","shell.execute_reply":"2024-05-23T07:49:40.907938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation","metadata":{"id":"DymhSATr3lCn","outputId":"b08163f6-79cd-42e3-e7d6-b4efac0d3f5d","execution":{"iopub.status.busy":"2024-05-23T07:49:40.911837Z","iopub.execute_input":"2024-05-23T07:49:40.912274Z","iopub.status.idle":"2024-05-23T07:49:40.956817Z","shell.execute_reply.started":"2024-05-23T07:49:40.912208Z","shell.execute_reply":"2024-05-23T07:49:40.955615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[0]","metadata":{"id":"ZHwP629fnTMD","outputId":"61636895-82a5-4c99-acaa-08e0d6d85d19","execution":{"iopub.status.busy":"2024-05-23T07:49:40.958867Z","iopub.execute_input":"2024-05-23T07:49:40.959368Z","iopub.status.idle":"2024-05-23T07:49:40.977489Z","shell.execute_reply.started":"2024-05-23T07:49:40.959323Z","shell.execute_reply":"2024-05-23T07:49:40.976072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text pre-processing","metadata":{"id":"z6XYa9-HXijc"}},{"cell_type":"markdown","source":"1. Tokenizer\n- Convert text to tokens with ids\n- Generate other inputs in the format the model requires\n\nfrom_pretrained:\n- gets tokenizer corr to our model\n- downloads and caches vocab used when trining this model","metadata":{"id":"YCZPHc0LXnXS"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"id":"R1svX9nCXY-g","execution":{"iopub.status.busy":"2024-05-23T07:49:40.978852Z","iopub.execute_input":"2024-05-23T07:49:40.979283Z","iopub.status.idle":"2024-05-23T07:49:42.578571Z","shell.execute_reply.started":"2024-05-23T07:49:40.979211Z","shell.execute_reply":"2024-05-23T07:49:42.577433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking that the tokenizer is a fast tokenizer as we will be using some of their features for preprocessing\ntokenizer.is_fast","metadata":{"id":"ex9WKYOiqxYE","outputId":"fa7462f3-c862-46fe-ae90-6b4344d5e245","execution":{"iopub.status.busy":"2024-05-23T07:49:42.579880Z","iopub.execute_input":"2024-05-23T07:49:42.580527Z","iopub.status.idle":"2024-05-23T07:49:42.589459Z","shell.execute_reply.started":"2024-05-23T07:49:42.580494Z","shell.execute_reply":"2024-05-23T07:49:42.588086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the tokenizer on a sample question, context\ntokenizer('What day is it today?', 'Today is a gloomy day, it most likely is a Wednesday')","metadata":{"id":"8QVLbktbrGjh","outputId":"da516513-a57b-4433-8836-6809358dd49e","execution":{"iopub.status.busy":"2024-05-23T07:49:42.594303Z","iopub.execute_input":"2024-05-23T07:49:42.594726Z","iopub.status.idle":"2024-05-23T07:49:42.606190Z","shell.execute_reply.started":"2024-05-23T07:49:42.594694Z","shell.execute_reply":"2024-05-23T07:49:42.605301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To handle very long contexts, split the context into multiple chunks with a sliding window between them\n\n# sample with very long context\nfor i, sample in enumerate(train):\n  # pass q, context to tokenizer & fetch input_ids\n  inp = tokenizer(sample['question'], sample['context'])['input_ids']\n  length = len(inp)\n  if length > 300: # model max length\n    long_sample = train[i]\n    break\n\n# long_sample, length","metadata":{"id":"KU5rBt4CrtXH","outputId":"5dc83a78-bf18-45bf-8092-2b361c645986","execution":{"iopub.status.busy":"2024-05-23T07:49:42.607630Z","iopub.execute_input":"2024-05-23T07:49:42.608011Z","iopub.status.idle":"2024-05-23T07:49:42.704860Z","shell.execute_reply.started":"2024-05-23T07:49:42.607978Z","shell.execute_reply":"2024-05-23T07:49:42.702988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pass the sample to tokenizer to cap only context lenght to 300\n\ntokenized_sample = tokenizer(\n    sample['question'],\n    sample['context'],\n    max_length = 300,\n    stride = 100,\n    truncation = 'only_second',\n    return_overflowing_tokens = True,\n)\ntokenized_sample\n","metadata":{"id":"NpqBteS9wcOd","outputId":"9bc2a49a-aa67-4ba8-e2e0-8106c85738a7","execution":{"iopub.status.busy":"2024-05-23T07:49:42.706372Z","iopub.execute_input":"2024-05-23T07:49:42.706886Z","iopub.status.idle":"2024-05-23T07:49:42.720105Z","shell.execute_reply.started":"2024-05-23T07:49:42.706836Z","shell.execute_reply":"2024-05-23T07:49:42.718658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1 sample is broken down into multiple chunks of length 300 with overlap.\n# input_ids therefore contains multiple lists of individual chunks\n\n# checkin that length is truncated to 300 for each chunk\n[len(x) for x in tokenized_sample['input_ids']]","metadata":{"id":"eso0U8Ui1Rxw","outputId":"c0a1f7ae-00dc-442a-f50a-5de88626274f","execution":{"iopub.status.busy":"2024-05-23T07:49:42.721581Z","iopub.execute_input":"2024-05-23T07:49:42.722999Z","iopub.status.idle":"2024-05-23T07:49:42.736705Z","shell.execute_reply.started":"2024-05-23T07:49:42.722950Z","shell.execute_reply":"2024-05-23T07:49:42.735049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's view the decoded tokenized output - context for 1 question has been split into multiple chunks with overlap\nfor chunk in tokenized_sample['input_ids']:\n  print(tokenizer.decode(chunk))","metadata":{"id":"gQ2dWI3G3wuV","outputId":"94d84a8b-2322-49a3-fa8b-d6f91cfb6866","execution":{"iopub.status.busy":"2024-05-23T07:49:42.739053Z","iopub.execute_input":"2024-05-23T07:49:42.739657Z","iopub.status.idle":"2024-05-23T07:49:42.758703Z","shell.execute_reply.started":"2024-05-23T07:49:42.739584Z","shell.execute_reply":"2024-05-23T07:49:42.757426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use return_offsets_mapping to return start and end chars for each token as output so that we can extract the ans\n# CLS token is a blank toke at (0,0) followed by 'how' at (0,3)\n\ntokenized_sample = tokenizer(\n    sample['question'],\n    sample['context'],\n    max_length = 300,\n    stride = 100,\n    truncation = 'only_second',\n    return_overflowing_tokens = True,\n    return_offsets_mapping = True,\n)\ntokenized_sample['offset_mapping'][0][:10]","metadata":{"id":"WLz53j1u3wr4","outputId":"570269a0-4c64-4af3-e820-a73dd02b3f7f","execution":{"iopub.status.busy":"2024-05-23T07:49:42.760069Z","iopub.execute_input":"2024-05-23T07:49:42.760480Z","iopub.status.idle":"2024-05-23T07:49:42.777827Z","shell.execute_reply.started":"2024-05-23T07:49:42.760448Z","shell.execute_reply":"2024-05-23T07:49:42.776758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verifiying that offset mapping generated actually corresponds to a word in input_ids (q+context)\n# 1st chunk, 1st word - how (skip CLS token)\n\nfirst_token_id = tokenized_sample['input_ids'][0][1]\nprint('First input id: ', first_token_id)\n\nprint('First word using input_ids: ',tokenizer.convert_ids_to_tokens(first_token_id))\n\noffset_id = tokenized_sample['offset_mapping'][0][1]\nprint('First offset id: ', offset_id)\n\nprint('First word using offset id: ', sample['question'][offset_id[0] : offset_id[1]])","metadata":{"id":"ndYXgnKA3wpU","outputId":"fdb045ac-8674-4ae7-e3b1-2f2477452304","execution":{"iopub.status.busy":"2024-05-23T07:49:42.779206Z","iopub.execute_input":"2024-05-23T07:49:42.780161Z","iopub.status.idle":"2024-05-23T07:49:42.788601Z","shell.execute_reply.started":"2024-05-23T07:49:42.780124Z","shell.execute_reply":"2024-05-23T07:49:42.786993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to distinguish between q & c in offset, use sequence_ids\n# None for CLS and SEP tokens\n# 0 for q\n# 1 for c\n\nseq_id = tokenized_sample.sequence_ids()\nprint(seq_id)","metadata":{"id":"UUtmUk4UACeJ","outputId":"dd45fe84-0b4a-4983-fb52-d645fa4d0677","execution":{"iopub.status.busy":"2024-05-23T07:49:42.790030Z","iopub.execute_input":"2024-05-23T07:49:42.791062Z","iopub.status.idle":"2024-05-23T07:49:42.801112Z","shell.execute_reply.started":"2024-05-23T07:49:42.791018Z","shell.execute_reply":"2024-05-23T07:49:42.799835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will now find the first and last token of the answer within the context\n# testing if ans is present in first chunk\n\nanswer = sample['answers']\ninput_ids = tokenized_sample['input_ids'][0]\noffset = tokenized_sample['offset_mapping'][0]\n\nans_start_char = answer['answer_start'][0]\nans_end_char   = ans_start_char + len(answer['text'][0])\n\n# get start and end char of current context\ncontext_start_char = 0\nwhile seq_id[context_start_char] != 1: # 1 indicates start of context\n  context_start_char += 1\ncontext_end_char = len(input_ids) - 1\nwhile seq_id[context_end_char] != 1: # read from end until first 1 is encountered -> end of context\n  context_end_char -= 1\n\nif offset[context_start_char][0] <= ans_start_char and  offset[context_end_char][1] >= ans_end_char : # if ans lies within current context\n  # take start and end token positions\n  # from the start of the context, move along context tokens until you reach the ans start char.\n  # context_start_char should not go beyond total length if ans is the last word\n  while context_start_char < len(offset) and offset[context_start_char][0] <= ans_start_char :\n    context_start_char += 1\n  chunk_ans_start_pos = context_start_char - 1\n  # move backwards from end of context until you reach the ans end char\n  while offset[context_end_char][1] >= ans_end_char :\n    context_end_char -= 1\n  chunk_ans_end_pos = context_end_char + 1\n  print('Ans start and end positions: ', chunk_ans_start_pos,chunk_ans_end_pos)\nelse:\n  print('Ans does not lie in current context (feature)')","metadata":{"id":"fon6MvVzACbr","outputId":"9a90cbf1-66b9-4414-d1d0-9611927c8963","execution":{"iopub.status.busy":"2024-05-23T07:49:42.802348Z","iopub.execute_input":"2024-05-23T07:49:42.803293Z","iopub.status.idle":"2024-05-23T07:49:42.816290Z","shell.execute_reply.started":"2024-05-23T07:49:42.803246Z","shell.execute_reply":"2024-05-23T07:49:42.814974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's decode this output and validate with the actual answer\n\nans_pos_decoded = tokenizer.decode(\n    tokenized_sample['input_ids'][0][chunk_ans_start_pos : chunk_ans_end_pos+1]\n)\nactual_ans = answer['text'][0]\n\nprint(ans_pos_decoded)\nprint(actual_ans)","metadata":{"id":"dVhGQODQACK6","outputId":"1da176fd-0cde-49a1-df60-a8df5d6869ae","execution":{"iopub.status.busy":"2024-05-23T07:49:42.817700Z","iopub.execute_input":"2024-05-23T07:49:42.818159Z","iopub.status.idle":"2024-05-23T07:49:42.835334Z","shell.execute_reply.started":"2024-05-23T07:49:42.818114Z","shell.execute_reply":"2024-05-23T07:49:42.834128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###Text pre-processing - Training data","metadata":{"id":"Chw5cBTJjbKc"}},{"cell_type":"code","source":"def preprocess_training_data(train_ds):\n  question = [ q.strip() for q in train_ds['question']] # Cleanup questions - Removing extra spaces\n  context  = [ c.strip() for c in train_ds['context']]\n  pad_on_right = tokenizer.padding_side == 'right' # returns True if question is followed by context,\n                                                   # to support models where context is followed by question, - order of question and context will be swapped\n\n  # To handle very long contexts, split the context into multiple chunks with a sliding window between them\n  inputs = tokenizer(\n    question if pad_on_right else context,\n    context if pad_on_right else question,\n    max_length = 384,                 # setting max length of question + context to 100\n    stride = 128,                     # overlapping tokens between chunks\n    truncation = 'only_second' if pad_on_right else 'only_first',       # if question +  context is too long, truncate only context (2nd argument)\n    return_overflowing_tokens = True, # return chunks that have been truncated as well\n    return_offsets_mapping = True,    # return which chunk contains the answer\n    padding = 'max_length',\n\t)\n\n  # One context is broken into multiple chunks if it exceeds max_length.\n  # Creating a mapper that maps each context to its corresponding features (chunks)\n  sample_mapping = inputs.pop('overflow_to_sample_mapping')\n\n\n  # Create offset map to map start and end end indices of answer in context\n  offset_mapping = inputs.pop('offset_mapping')\n\n\n  # Now that context is split into chunks, we will now find the first and last token of the answer within the context\n  chunk_ans_start_pos = []\n  chunk_ans_end_pos = []\n\n  # Now that context is split into chunks, we will now find the first and last token of the answer within the context\n\n  chunk_ans_start_pos = []\n  chunk_ans_end_pos = []\n\n  for i, offset in enumerate(offset_mapping):\n\n\n    # Fetch input ids and CLS index\n    input_ids = inputs['input_ids'][i]\n    cls_index = input_ids.index(tokenizer.cls_token_id)\n\n    # calculate context start and end indices\n    # fetch sequence_ids to know what is the question and what is the context\n    seq_id = inputs.sequence_ids(i)\n\n    sample_idx = sample_mapping[i] # get index of current chunk\n    answer = train_ds['answers'][sample_idx] # get answers for each source\n\n    # if no answer is given, set cls_index as start and end char\n    if len(answer['answer_start']) == 0:\n      chunk_ans_start_pos.append(cls_index)\n      chunk_ans_end_pos.append(cls_index)\n    else:\n      # calculate ans start and end indices\n      ans_start_char = answer['answer_start'][0]\n      ans_end_char   = ans_start_char + len(answer['text'][0])\n\n\n      # get start and end char of current context\n      context_start_char = 0\n      while seq_id[context_start_char] != (1 if pad_on_right else 0): # 1 indicates start of context\n        context_start_char += 1\n\n      context_end_char = len(input_ids) - 1\n      while seq_id[context_end_char] != (1 if pad_on_right else 0): # continue reading 1's until 0 is encountered -> end of context\n        context_end_char -= 1\n\n\n      # for all the chunks, check if ans lies within context of that chunk\n      # if current context doesn't contain ans -> set cls_index\n      if not(\n          offset[context_start_char][0] <= ans_start_char and  offset[context_end_char][1] >= ans_end_char # checking if ans lies within current context\n      ):\n        chunk_ans_start_pos.append(cls_index)\n        chunk_ans_end_pos.append(cls_index)\n\n      else: # take start and end token positions\n        # from the start of the context, move along context tokens until you reach the ans start char.\n        # context_start_char should not go beyond total length if ans is the last word\n        while context_start_char < len(offset) and offset[context_start_char][0] <= ans_start_char:\n          context_start_char += 1\n        chunk_ans_start_pos.append(context_start_char - 1)\n\n        # move backwards from end of context until you reach the ans end char\n        while offset[context_end_char][1] >= ans_end_char:\n          context_end_char -= 1\n        chunk_ans_end_pos.append(context_end_char + 1)\n\n  # Add start and end positions to inputs\n  inputs['start_positions'] = chunk_ans_start_pos\n  inputs['end_positions'] = chunk_ans_end_pos\n\n  return inputs\n","metadata":{"id":"FH81fAkuenF-","execution":{"iopub.status.busy":"2024-05-23T07:49:42.837567Z","iopub.execute_input":"2024-05-23T07:49:42.838065Z","iopub.status.idle":"2024-05-23T07:49:42.857692Z","shell.execute_reply.started":"2024-05-23T07:49:42.838024Z","shell.execute_reply":"2024-05-23T07:49:42.856462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply the function to the entire dataset","metadata":{"id":"Wr9tjs2lbXUg"}},{"cell_type":"code","source":"processed_train = train.map(\n    preprocess_training_data,\n    batched=True, # 1 row is being split into multiple chunks/ features\n    remove_columns = train.column_names,\n)\n\nprocessed_validation = validation.map(\n    preprocess_training_data,\n    batched=True, # 1 row is being split into multiple chunks/ features\n    remove_columns = train.column_names,\n)\n\nprint('\\n')\nprint('Number of records in original training data: ', len(train))\nprint('Number of records in processed training data: ',len(processed_train))\nprint('\\n')\nprint('Number of records in original validation data: ',len(validation))\nprint('Number of records in processed training data: ', len(processed_validation))","metadata":{"id":"1u3pmYR6Nqlq","outputId":"f5f0ab5f-1b2f-40c7-e814-76739bb02eab","execution":{"iopub.status.busy":"2024-05-23T07:49:42.859719Z","iopub.execute_input":"2024-05-23T07:49:42.860428Z","iopub.status.idle":"2024-05-23T07:49:43.253615Z","shell.execute_reply.started":"2024-05-23T07:49:42.860394Z","shell.execute_reply":"2024-05-23T07:49:43.252288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_train","metadata":{"id":"-kfASClVhZ_s","outputId":"91383b2d-6045-41c0-f873-f6744b07adcb","execution":{"iopub.status.busy":"2024-05-23T07:49:43.255462Z","iopub.execute_input":"2024-05-23T07:49:43.256277Z","iopub.status.idle":"2024-05-23T07:49:43.264277Z","shell.execute_reply.started":"2024-05-23T07:49:43.256217Z","shell.execute_reply":"2024-05-23T07:49:43.263096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_validation","metadata":{"id":"BEg7ScqJhmFF","outputId":"fc813323-8150-4473-9249-4188d8cbf475","execution":{"iopub.status.busy":"2024-05-23T07:49:43.266159Z","iopub.execute_input":"2024-05-23T07:49:43.266606Z","iopub.status.idle":"2024-05-23T07:49:43.278640Z","shell.execute_reply.started":"2024-05-23T07:49:43.266568Z","shell.execute_reply":"2024-05-23T07:49:43.277436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ## ------------------------------- Pre-processing Complete ------------------------------------","metadata":{"id":"m027HLxaqtfL"}},{"cell_type":"markdown","source":"### Fine-tuning the model\nTraining the model","metadata":{"id":"Vr-hKYCUwrJb"}},{"cell_type":"code","source":"# Initialize the model\n\nfrom transformers import TFAutoModelForQuestionAnswering\n\n# Using from_pretrained to download and cache the model\nmodel = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","metadata":{"id":"i9r5Nz0Tl2tS","outputId":"d3ad4a3a-26eb-4e7c-f707-e6e59d49b9d4","execution":{"iopub.status.busy":"2024-05-23T07:49:43.282134Z","iopub.execute_input":"2024-05-23T07:49:43.282533Z","iopub.status.idle":"2024-05-23T07:49:47.300511Z","shell.execute_reply.started":"2024-05-23T07:49:43.282502Z","shell.execute_reply":"2024-05-23T07:49:47.299039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up training parameters\nmodel_name = model_checkpoint.split('/')[-1]\nhuggingface_model_name = f'{model_name}-finetuned-squad'\nlearning_rate = 2e-5\nnum_epochs = 1","metadata":{"id":"XB9d-9LvYI7-","execution":{"iopub.status.busy":"2024-05-23T07:49:47.303464Z","iopub.execute_input":"2024-05-23T07:49:47.303982Z","iopub.status.idle":"2024-05-23T07:49:47.310898Z","shell.execute_reply.started":"2024-05-23T07:49:47.303937Z","shell.execute_reply":"2024-05-23T07:49:47.309505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert dataset to tf datasets\ntrain_ds = model.prepare_tf_dataset(\n    processed_train,\n    shuffle = True,\n    batch_size = batch_size\n)\n\nvalidation_ds = model.prepare_tf_dataset(\n    processed_validation,\n    shuffle = False,\n    batch_size = batch_size\n)\n\n","metadata":{"id":"_IrCoJlIImwA","execution":{"iopub.status.busy":"2024-05-23T07:49:47.312107Z","iopub.execute_input":"2024-05-23T07:49:47.312523Z","iopub.status.idle":"2024-05-23T07:49:47.935720Z","shell.execute_reply.started":"2024-05-23T07:49:47.312491Z","shell.execute_reply":"2024-05-23T07:49:47.934668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create optimizer from trnasformers uses AdamW optimizer with weight decay\nfrom transformers import create_optimizer\n\ntraining_steps = len(train_ds) * num_epochs\n\noptimizer, _ = create_optimizer(\n    init_lr = learning_rate,\n    num_train_steps = training_steps,\n    num_warmup_steps = 0\n)\n","metadata":{"id":"1hDeWITczeu-","execution":{"iopub.status.busy":"2024-05-23T07:49:47.937147Z","iopub.execute_input":"2024-05-23T07:49:47.937528Z","iopub.status.idle":"2024-05-23T07:49:47.966235Z","shell.execute_reply.started":"2024-05-23T07:49:47.937498Z","shell.execute_reply":"2024-05-23T07:49:47.965237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compile the model \n# no need to mention loss as model automatically handles it\n\nimport tensorflow as tf\nkeras = tf.keras\n\nmodel.compile(optimizer = optimizer, metrics = ['accuracy'])","metadata":{"id":"WDZ4cJd8cZgH","execution":{"iopub.status.busy":"2024-05-23T07:49:47.972184Z","iopub.execute_input":"2024-05-23T07:49:47.972715Z","iopub.status.idle":"2024-05-23T07:49:47.994157Z","shell.execute_reply.started":"2024-05-23T07:49:47.972670Z","shell.execute_reply":"2024-05-23T07:49:47.992796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T07:49:47.996032Z","iopub.execute_input":"2024-05-23T07:49:47.996462Z","iopub.status.idle":"2024-05-23T07:49:48.033182Z","shell.execute_reply.started":"2024-05-23T07:49:47.996428Z","shell.execute_reply":"2024-05-23T07:49:48.031850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define callbacks\nfrom transformers.keras_callbacks import PushToHubCallback\n\npush_to_hub_callback = PushToHubCallback(\n    output_dir=\"./qa_model_save\",\n    tokenizer=tokenizer,\n    hub_model_id=huggingface_model_name,\n)\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./qa_model_save/logs\")\n\ncallbacks = [push_to_hub_callback, tensorboard_callback]","metadata":{"execution":{"iopub.status.busy":"2024-05-23T07:49:55.523962Z","iopub.execute_input":"2024-05-23T07:49:55.524497Z","iopub.status.idle":"2024-05-23T07:51:49.649437Z","shell.execute_reply.started":"2024-05-23T07:49:55.524457Z","shell.execute_reply":"2024-05-23T07:51:49.648217Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the pre-trained weights\ncheckpoint_path = '/kaggle/working/qa_model_save/tf_model.h5'\nmodel.load_weights(checkpoint_path)\n\n\n# Train the model on the entire dataset\n# model.fit(train_ds,\n#           validation_data = validation_ds,\n#           epochs = num_epochs,\n#           callbacks = callbacks,\n#           )\n\n# model.save_weights(\"full_model_trained.h5\")","metadata":{"id":"cFf-1FrNwBDR","outputId":"7651d32b-1dc0-4c91-d860-b4bbdc77e7e8","execution":{"iopub.status.busy":"2024-05-23T08:03:38.405144Z","iopub.execute_input":"2024-05-23T08:03:38.405599Z","iopub.status.idle":"2024-05-23T08:03:39.053443Z","shell.execute_reply.started":"2024-05-23T08:03:38.405569Z","shell.execute_reply":"2024-05-23T08:03:39.052042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make predictions and evaluate metrics on validation dataset","metadata":{}},{"cell_type":"code","source":"# Checking model output for a batch of validataion data\n\nbatch = next(iter(validation_ds))\npredictions = model.predict_on_batch(batch)\npredictions.keys()\n\n# start_logits is an array of lists that gives the probability of each token being the start of the ans.\n# so to find the answer for each feature, take the max of start start_logits as starting position","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We have start and end logit (probabilities) for each feature (16) and each token (384) \npredictions.start_logits.shape, predictions.end_logits.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taking max probability to find ans start and end indices\nimport numpy as np\nnp.argmax(predictions.start_logits, -1), np.argmax(predictions.end_logits, -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# since we take max, there may be cases where the start position is greater than the end position;\n# so we will take the top 20 best predictions\n# - > check if each one is valid\n# - > sort them by their score (start logit + end logit) and keep the best one\n\n# Testing this for the 1st feature\n\nimport numpy as np\n\nn_best_logits = 20\n\nstart_logit = predictions.start_logits[0]\nend_logit   = predictions.end_logits[0]\n\n# list of n_best indexes\nstart_indexes = np.argsort(start_logit)[-1: -n_best_logits-1: -1].tolist()\nend_indexes   = np.argsort(end_logit)[-1: -n_best_logits-1: -1].tolist()\n\nanswers = []\nfor start_idx in start_indexes:\n      for end_idx in end_indexes:\n        # check if ans is fully in the context else skip\n        if start_idx <= end_idx:\n          answers.append(\n              {\n                  'score': start_logit[start_idx] + end_logit[end_idx],\n                  'text' : ''\n              }\n          )\n\nanswers[:10]\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to fetch the text, we need to re-process the validation_ds and add:\n# 1. record_id to map each record to its corr features\n# 2. offset map - gives start and end chars of each token\n\ndef process_validation_data(val_ds):\n  question = [ q.strip() for q in val_ds['question']] # Cleanup questions - Removing extra spaces\n  context  = [ c.strip() for c in val_ds['context']]\n  pad_on_right = tokenizer.padding_side == 'right'\n\n  # To handle very long contexts, split the context into multiple chunks with a sliding window between them\n  inputs = tokenizer(\n    question if pad_on_right else context,\n    context if pad_on_right else question,\n    max_length = 384, # setting max length of question + context to 384\n    stride = 128, # overlapping tokens between chunks\n    truncation = 'only_second' if pad_on_right else 'only_first',\n    return_overflowing_tokens = True,\n    return_offsets_mapping = True, # return start and end indices of answer in context\n    padding = 'max_length',  # padding to max length as contexts are long and no need for dynamic padding\n    )\n\n  # One context is broken into multiple chunks if it exceeds max_length.\n  # Creating a mapper that maps each context to its corresponding features (chunks)\n  sample_mapping = inputs.pop('overflow_to_sample_mapping')\n\n  # cleanup offset mapping: it contains offset for question + context\n  # set question offset to None so that we can identify Question and Context\n  inputs['record_id']=[]\n  for i in range(len(inputs['input_ids'])):\n    # Take seq ids to distinguish b/w q and c\n    seq_id = inputs.sequence_ids(i)\n    context_index = 1 if pad_on_right else 0\n\n    # Fetching current feature\n    sample_idx = sample_mapping[i]\n    # Add record_ids to inputs\n    inputs['record_id'].append(val_ds['id'][sample_idx]) # create record id list to tie created chunks back to their source\n\n\n    # update offset mapping to None for everything not part of context : seq_id is 1 for context and 0 for question\n    inputs['offset_mapping'][i] = [\n        (offset if seq_id[j] == context_index else None)\n        for j, offset in enumerate(inputs['offset_mapping'][i])\n    ]\n\n\n  return inputs\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reprocess the validation data\nreprocessed_validation = validation.map(\n    process_validation_data,\n    batched = True,\n    remove_columns = validation.column_names,\n)\n\n# Convert it to tf dataset\nreprocessed_validation_tf = model.prepare_tf_dataset(\n    reprocessed_validation,\n    shuffle = False,\n    batch_size = batch_size,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions\n\npred_val = model.predict(reprocessed_validation_tf)\npred_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# modifying the earlier test for 1st feature\nimport numpy as np\n\nn_best_logits = 20\nmax_ans_length = 30 # eliminate long ans\n\nstart_logit = predictions.start_logits[0]\nend_logit   = predictions.end_logits[0]\noffsets = reprocessed_validation[0]['offset_mapping']\ncontext =  validation[0]['context'] # context of 1st feature\n\n# list of n_best indexes\nstart_indexes = np.argsort(start_logit)[-1: -n_best_logits-1: -1].tolist()\nend_indexes   = np.argsort(end_logit)[-1: -n_best_logits-1: -1].tolist()\n\nanswers = []\nfor start_idx in start_indexes:\n      for end_idx in end_indexes:\n        # check if 1. indices are out of bounds and 2. indices are not part of context. None = question/ CLS token\n        if start_idx >= len(offsets) or end_idx >= len(offsets) \\\n        or offsets[start_idx] is None or offsets[end_idx] is None:\n          continue\n        # check if ans length is not < 0 or > maxlength then skip\n        if end_idx < start_idx or end_idx - start_idx + 1 > max_ans_length:\n          continue\n        if start_idx <= end_idx:\n          ans_start_char = offsets[start_idx][0]\n          ans_end_char   = offsets[end_idx][1]\n          answers.append(\n              {\n                  'score': start_logit[start_idx] + end_logit[end_idx],\n                  'text' : context[ans_start_char : ans_end_char]\n              }\n          )\n# top 20 answers sorted by score\nsorted_answers = sorted(answers, key = lambda x: x['score'], reverse = True)[:n_best_logits]\nsorted_answers, len(sorted_answers)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check with the actual answer for 1st feature\n\nvalidation[0]['answers']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model's highest probability answer matches the actual answer!","metadata":{}},{"cell_type":"markdown","source":"#### Post-processing predictions","metadata":{}},{"cell_type":"markdown","source":"Each example has multiple features. Mapping each example to its corresponding features to extract the answer","metadata":{}},{"cell_type":"code","source":"import collections\n\nfeatures_per_example = collections.defaultdict(list)\n\nexample_id_index_map = { k: i  for i, k in enumerate(validation['id']) }\n\nfor i, feature in enumerate(reprocessed_validation):\n  features_per_example[example_id_index_map[feature['record_id']]].append(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Track the progress of code execution with a progress bar\nfrom tqdm.auto import tqdm\n\ndef postprocess_predictions(examples, features, pred_start_logits, pred_end_logits):\n    # Mapping each example to its corresponding features \n    features_per_example = collections.defaultdict(list)\n    example_id_index_map = { k: i  for i, k in enumerate(examples['id']) }\n\n    for i, feature in enumerate(features):\n         features_per_example[example_id_index_map[feature['record_id']]].append(i)\n    \n    # Display number of examples and features\n    print(f'Post processing: {len(examples)} examples split into {len(features)} features')\n        \n    # Create empty dict for storing predictions\n    predicted_ans = collections.OrderedDict()\n    \n    # Looping over all examples\n    for i, example in enumerate(tqdm(examples)):\n        # fetch features associated to current example\n        features_indices = features_per_example[i]\n        context = example['context']\n        answers = []\n        # For squadv2 - handling impossible answers\n        min_null_score = None\n        \n        # Looping over all features associated to current example\n        for feature_index in features_indices:\n            # get the model predictions and offsets for this feature\n            start_logits = pred_start_logits[feature_index]\n            end_logits   = pred_end_logits[feature_index]\n            offsets      = features[feature_index]['offset_mapping']\n            \n            # squad_v2 Impossible answers: update minimum null prediction\n            # for impossible ans, start and end index = CLS token index\n            cls_index = features[feature_index]['input_ids'].index(tokenizer.cls_token_id)\n            # compute the score for impossible ans for this feature:\n            # 1. ans is simply not in the current feature => not an impossible ans\n            # 2. it is truly an impossible ans\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            # get the best impossible ans score for an example\n            # update the null score if impossible ans score for this feature > prev impossible ans score\n            if min_null_score is None or feature_null_score > min_null_score:\n                min_null_score = feature_null_score\n                \n            \n            # list of n_best indexes\n            n_best_size=20\n            max_answer_length=30\n            start_indexes = np.argsort(start_logits)[-1: -n_best_logits-1: -1].tolist()\n            end_indexes   = np.argsort(end_logits)[-1: -n_best_logits-1: -1].tolist()\n\n            for start_idx in start_indexes:\n                for end_idx in end_indexes:\n                    # check if indices are within bounds or ans is fully in the context else skip\n                    if  (start_idx >= len(offsets) or end_idx >= len(offsets) or \n                    offsets[start_idx] is None or offsets[end_idx] is None):\n                        continue\n                    # check if ans length is not < 0 or > maxlength then skip\n                    if end_idx < start_idx or end_idx - start_idx + 1 > max_ans_length:\n                        continue\n                    start_char = offsets[start_idx][0]\n                    end_char   = offsets[end_idx][1]\n                    answers.append(\n                    {\n                        'text' : context[start_char : end_char],\n                        'score': start_logits[start_idx] + end_logits[end_idx]\n                    }\n                    )   \n                \n        if len(answers)>0:\n            best_ans = max(answers, key = lambda x: x['score']) #selecting ans with highest logit score as best\n        else:\n            best_ans = {'text': '', score: 0.0}\n        \n        # calculate the final ans: best ans or null ans (for squad_v2)\n        if not squad_v2:\n            predicted_ans[example['id']] = best_ans['text']\n        else: # if normal ans score > impossible ans score take normal ans else blank\n            final_ans = best_ans['text'] if best_ans['score'] > min_null_score else ''\n            predicted_ans[example['id']] = final_ans\n    return predicted_ans\n\n# Apply post preoccsing function to validation predicted_ans\nfinal_predictions = postprocess_predictions(\n    validation,\n    reprocessed_validation,\n    pred_val['start_logits'],\n    pred_val['end_logits'],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This format of predicted ans is expected by the squad evaluation metric we will use.","metadata":{}},{"cell_type":"markdown","source":"#### Load the squad evaluation metric","metadata":{}},{"cell_type":"code","source":"metric = load_metric('squad_v2' if squad_v2 else 'squad')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Formatting the predicted ans to a list of dict as expected by the squad evaluation metric we will use.","metadata":{}},{"cell_type":"code","source":"if squad_v2:\n    formatted_predictions = [\n        {'id': key, 'prediction_text': value, 'no_answer_probability':0.0} # no_ans_prob=0 since we explicity set it to blank for no ans\n        for key, value in final_predictions.items()\n    ]\nelse:\n    formatted_predictions = [\n        {'id': key, 'prediction_text': value}\n        for key, value in final_predictions.items()\n    ]\n    \nactual_ans = [\n    {'id': example['id'], 'answers': example['answers']}\n    for example in validation\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View sample predicted and actual ans\nprint(formatted_predictions[4])\nprint(actual_ans[4])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evalute using the metric\nmetric.compute(predictions = formatted_predictions, references = actual_ans)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Making Inferences","metadata":{}},{"cell_type":"code","source":"# Downloading our model from the hub\n\nfrom transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n\ncheckpoint = \"Mariah64/distilbert-base-uncased-finetuned-squad\"\nmodel      = TFAutoModelForQuestionAnswering.from_pretrained(checkpoint)\ntokenizer  = AutoTokenizer.from_pretrained(checkpoint)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:06:10.088709Z","iopub.execute_input":"2024-05-23T08:06:10.089173Z","iopub.status.idle":"2024-05-23T08:06:12.108062Z","shell.execute_reply.started":"2024-05-23T08:06:10.089137Z","shell.execute_reply":"2024-05-23T08:06:12.106698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing with random data\n\ncontext = \"\"\"The dominant sequence transduction models are based on complex recurrent or convolutional \nneural networks in an encoder-decoder configuration. The best performing models also connect the encoder\nand decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, \nbased solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on \ntwo machine translation tasks show these models to be superior in quality while being more parallelizable \nand requiring significantly less time to train.\"\"\"\nquestion = \"What kind of mechanisms is Transformer based on?\"\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:08:39.135644Z","iopub.execute_input":"2024-05-23T08:08:39.136117Z","iopub.status.idle":"2024-05-23T08:08:39.142249Z","shell.execute_reply.started":"2024-05-23T08:08:39.136082Z","shell.execute_reply":"2024-05-23T08:08:39.141279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ninputs = tokenizer([context], [question], return_tensors='np')\noutputs = model(inputs)\n\n# Finding best possible ans\nans_start = np.argmax(outputs.start_logits[0])\nans_end   = np.argmax(outputs.end_logits[0])\nprint('ans_start = ', ans_start)\nprint('ans_end = ', ans_end)\n\n# Extract ans tokens between start and end positions\nans = inputs['input_ids'][0, ans_start : ans_end + 1]\nprint('ans tokens = ', ans)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:08:42.468927Z","iopub.execute_input":"2024-05-23T08:08:42.469375Z","iopub.status.idle":"2024-05-23T08:08:42.828880Z","shell.execute_reply.started":"2024-05-23T08:08:42.469341Z","shell.execute_reply":"2024-05-23T08:08:42.827639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decoding the tokens back to text\ntokenizer.decode(ans)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:08:50.440878Z","iopub.execute_input":"2024-05-23T08:08:50.441291Z","iopub.status.idle":"2024-05-23T08:08:50.450973Z","shell.execute_reply.started":"2024-05-23T08:08:50.441261Z","shell.execute_reply":"2024-05-23T08:08:50.449520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Using Pipline API for quick inferencing\nOnce the model is on the hub, we can use pipeline api to replace the above steps and give us the ans directly","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nqa = pipeline('question-answering',  \"Mariah64/distilbert-base-uncased-finetuned-squad\", framework='tf')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:05:25.597581Z","iopub.execute_input":"2024-05-23T08:05:25.597975Z","iopub.status.idle":"2024-05-23T08:05:35.316921Z","shell.execute_reply.started":"2024-05-23T08:05:25.597945Z","shell.execute_reply":"2024-05-23T08:05:35.314921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = '''\nThe advent of the accordion is the subject of debate among researchers. Many credit C. Friedrich L. Buschmann, whose Handäoline was patented \nin Berlin in 1822, as the inventor of the accordion, while others give the distinction to Cyril Demian of Vienna, who patented his Accordion \nin 1829, thus coining the name. A modification of the Handäoline, Demian’s invention comprised a small manual bellows and five keys, although, \nas Demian noted in a description of the instrument, extra keys could be incorporated into the design. Numerous variations of the device soon followed.\n'''\n\nquestion = 'Whats is the subject of debate among researchers'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qa(context = context, question = question)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T08:05:35.319145Z","iopub.execute_input":"2024-05-23T08:05:35.319665Z","iopub.status.idle":"2024-05-23T08:05:35.837064Z","shell.execute_reply.started":"2024-05-23T08:05:35.319631Z","shell.execute_reply":"2024-05-23T08:05:35.835446Z"},"trusted":true},"execution_count":null,"outputs":[]}]}