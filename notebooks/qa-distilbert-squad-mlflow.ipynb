{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa53ad",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-05-31T16:23:52.861992Z",
     "iopub.status.busy": "2024-05-31T16:23:52.861643Z",
     "iopub.status.idle": "2024-05-31T16:24:10.040704Z",
     "shell.execute_reply": "2024-05-31T16:24:10.039783Z"
    },
    "id": "14fa53ad",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "10e05de8-6c36-4088-897d-d10e318b1a73",
    "papermill": {
     "duration": 17.213695,
     "end_time": "2024-05-31T16:24:10.043023",
     "exception": false,
     "start_time": "2024-05-31T16:23:52.829328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\r\n",
      "  Downloading mlflow-2.13.1-py3-none-any.whl.metadata (29 kB)\r\n",
      "Requirement already satisfied: Flask<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.0.3)\r\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.13.1)\r\n",
      "Collecting cachetools<6,>=5.0.0 (from mlflow)\r\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.2.1)\r\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (7.0.0)\r\n",
      "Requirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.10/site-packages (from mlflow) (0.4)\r\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.1.41)\r\n",
      "Collecting graphene<4 (from mlflow)\r\n",
      "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\r\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (6.11.0)\r\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.5.2)\r\n",
      "Requirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.7.5)\r\n",
      "Requirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.26.4)\r\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.22.0)\r\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.22.0)\r\n",
      "Requirement already satisfied: packaging<25 in /opt/conda/lib/python3.10/site-packages (from mlflow) (21.3)\r\n",
      "Requirement already satisfied: pandas<3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.2.1)\r\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.20.3)\r\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (14.0.2)\r\n",
      "Requirement already satisfied: pytz<2025 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2023.3.post1)\r\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.10/site-packages (from mlflow) (6.0.1)\r\n",
      "Collecting querystring-parser<2 (from mlflow)\r\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\r\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.31.0)\r\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.2.2)\r\n",
      "Requirement already satisfied: scipy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.11.4)\r\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.0.25)\r\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (0.4.4)\r\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.1.2)\r\n",
      "Collecting gunicorn<23 (from mlflow)\r\n",
      "  Downloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\r\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\r\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (4.9.0)\r\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.18)\r\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (3.0.3)\r\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (2.2.0)\r\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (1.8.2)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\r\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\r\n",
      "  Downloading graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\r\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting aniso8601<10,>=8 (from graphene<4->mlflow)\r\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.5)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.9.0.post0)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.0.0->mlflow) (1.2.14)\r\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.0.0->mlflow) (0.43b0)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3->mlflow) (2023.4)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from querystring-parser<2->mlflow) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (2024.2.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.2.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.0.0->mlflow) (1.14.1)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.1)\r\n",
      "Downloading mlflow-2.13.1-py3-none-any.whl (25.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.0/25.0 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\r\n",
      "Downloading graphene-3.3-py2.py3-none-any.whl (128 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\r\n",
      "Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\r\n",
      "\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\r\n",
      "\u001b[0mInstalling collected packages: aniso8601, querystring-parser, graphql-core, cachetools, gunicorn, graphql-relay, graphene, mlflow\r\n",
      "  Attempting uninstall: cachetools\r\n",
      "    Found existing installation: cachetools 4.2.4\r\n",
      "    Uninstalling cachetools-4.2.4:\r\n",
      "      Successfully uninstalled cachetools-4.2.4\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 24.4.1 requires cubinlinker, which is not installed.\r\n",
      "cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cudf 24.4.1 requires ptxcompiler, which is not installed.\r\n",
      "cuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "cudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed aniso8601-9.0.1 cachetools-5.3.2 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-22.0.0 mlflow-2.13.1 querystring-parser-1.2.4\r\n"
     ]
    }
   ],
   "source": [
    "! pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c66a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T16:24:10.114799Z",
     "iopub.status.busy": "2024-05-31T16:24:10.114439Z",
     "iopub.status.idle": "2024-05-31T16:24:10.666229Z",
     "shell.execute_reply": "2024-05-31T16:24:10.665039Z"
    },
    "id": "d06c66a6",
    "outputId": "c9685b73-2851-4671-c8ec-28a7d8310615",
    "papermill": {
     "duration": 0.5901,
     "end_time": "2024-05-31T16:24:10.668189",
     "exception": false,
     "start_time": "2024-05-31T16:24:10.078089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, logout\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05ca89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T16:24:10.737210Z",
     "iopub.status.busy": "2024-05-31T16:24:10.736425Z",
     "iopub.status.idle": "2024-05-31T16:24:15.694998Z",
     "shell.execute_reply": "2024-05-31T16:24:15.693933Z"
    },
    "id": "df05ca89",
    "outputId": "fe8ad86e-162e-435c-8e01-03759e21eef1",
    "papermill": {
     "duration": 4.995393,
     "end_time": "2024-05-31T16:24:15.697527",
     "exception": false,
     "start_time": "2024-05-31T16:24:10.702134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "git-lfs is already the newest version (2.9.2-1).\r\n",
      "0 upgraded, 0 newly installed, 0 to remove and 75 not upgraded.\r\n"
     ]
    }
   ],
   "source": [
    "! apt install git-lfs\n",
    "! git config --global user.email \"mariahferns@gmail.com\"\n",
    "! git config --global user.name \"Mariah\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d1bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T16:24:15.768862Z",
     "iopub.status.busy": "2024-05-31T16:24:15.768505Z",
     "iopub.status.idle": "2024-05-31T16:24:35.777675Z",
     "shell.execute_reply": "2024-05-31T16:24:35.776864Z"
    },
    "id": "8c6d1bdd",
    "outputId": "3e376a94-d9ed-4510-fddd-d872f94eaccc",
    "papermill": {
     "duration": 20.047586,
     "end_time": "2024-05-31T16:24:35.780126",
     "exception": false,
     "start_time": "2024-05-31T16:24:15.732540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 16:24:21.979143: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-31 16:24:21.979258: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-31 16:24:22.117577: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "from transformers import pipeline\n",
    "import transformers\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "import warnings\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature # stores basic information about model, metadata about the model\n",
    "import logging\n",
    "from urllib.parse import urlparse # to access mlflow ui using http\n",
    "\n",
    "\n",
    "# Load dataset and metric for evaluation\n",
    "from datasets import load_dataset, load_metric\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d6b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T16:24:35.854197Z",
     "iopub.status.busy": "2024-05-31T16:24:35.852016Z",
     "iopub.status.idle": "2024-05-31T16:24:35.890356Z",
     "shell.execute_reply": "2024-05-31T16:24:35.889455Z"
    },
    "id": "e63d6b24",
    "papermill": {
     "duration": 0.076832,
     "end_time": "2024-05-31T16:24:35.892406",
     "exception": false,
     "start_time": "2024-05-31T16:24:35.815574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Setup initial parameters\n",
    "mlflow.tensorflow.autolog()\n",
    "squad_v2 = False # indicates if impossible answers are allowed\n",
    "model_checkpoint = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb19a8a",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d38aba3827f249d0a1cff9b4a67a0e2b",
      "01e0b0d1fb9c495caae8d98d34091aa3",
      "f3b2d5d2ac2d436b9a81e460753a6aa4",
      "8c944b9addbc4476a118a6824d73e7b8",
      "484c9f7f582f429e930a8e876af6a329"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-05-31T16:24:35.963248Z",
     "iopub.status.busy": "2024-05-31T16:24:35.962916Z",
     "iopub.status.idle": "2024-05-31T16:24:46.059194Z",
     "shell.execute_reply": "2024-05-31T16:24:46.058434Z"
    },
    "id": "efb19a8a",
    "outputId": "45c25adb-728d-4d57-e810-52b6830faaa5",
    "papermill": {
     "duration": 10.133911,
     "end_time": "2024-05-31T16:24:46.061104",
     "exception": false,
     "start_time": "2024-05-31T16:24:35.927193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38aba3827f249d0a1cff9b4a67a0e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e0b0d1fb9c495caae8d98d34091aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b2d5d2ac2d436b9a81e460753a6aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c944b9addbc4476a118a6824d73e7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484c9f7f582f429e930a8e876af6a329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the SQuAD dataset\n",
    "\n",
    "# use a smaller size for testing\n",
    "train_size = 87599\n",
    "valid_size = 10570\n",
    "\n",
    "split = [\n",
    "    f\"train[:{train_size}]\",\n",
    "    f\"validation[:{valid_size}]\"\n",
    "]\n",
    "\n",
    "\n",
    "train, validation = load_dataset('squadv2' if squad_v2 else 'squad', split=split)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6addcd",
   "metadata": {
    "id": "1c6addcd",
    "papermill": {
     "duration": 0.035054,
     "end_time": "2024-05-31T16:24:46.133447",
     "exception": false,
     "start_time": "2024-05-31T16:24:46.098393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03705a63",
   "metadata": {
    "id": "03705a63",
    "papermill": {
     "duration": 0.034851,
     "end_time": "2024-05-31T16:24:46.203688",
     "exception": false,
     "start_time": "2024-05-31T16:24:46.168837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. Tokenizer\n",
    "- Convert text to tokens with ids\n",
    "- Generate other inputs in the format the model requires\n",
    "\n",
    "from_pretrained:\n",
    "- gets tokenizer corr to our model\n",
    "- downloads and caches vocab used when trining this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523b951",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b82faab76a6d43549dd3d03358acafca",
      "fec7df47ff114df5bedc24af54263f2d",
      "0f5570b8b04e4529b75a496fcd5c5bfa",
      "1d3546a6b79b42aaad3b7f3d0eb90acb"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-05-31T16:24:46.276363Z",
     "iopub.status.busy": "2024-05-31T16:24:46.275982Z",
     "iopub.status.idle": "2024-05-31T16:24:47.803755Z",
     "shell.execute_reply": "2024-05-31T16:24:47.802981Z"
    },
    "id": "8523b951",
    "outputId": "93c698cb-1578-439c-dc1d-289583aedc53",
    "papermill": {
     "duration": 1.567138,
     "end_time": "2024-05-31T16:24:47.806045",
     "exception": false,
     "start_time": "2024-05-31T16:24:46.238907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82faab76a6d43549dd3d03358acafca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec7df47ff114df5bedc24af54263f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5570b8b04e4529b75a496fcd5c5bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3546a6b79b42aaad3b7f3d0eb90acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbbcd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T16:24:47.880719Z",
     "iopub.status.busy": "2024-05-31T16:24:47.879930Z",
     "iopub.status.idle": "2024-05-31T16:24:47.886136Z",
     "shell.execute_reply": "2024-05-31T16:24:47.885319Z"
    },
    "id": "dddbbcd2",
    "outputId": "fa7462f3-c862-46fe-ae90-6b4344d5e245",
    "papermill": {
     "duration": 0.045548,
     "end_time": "2024-05-31T16:24:47.888200",
     "exception": false,
     "start_time": "2024-05-31T16:24:47.842652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking that the tokenizer is a fast tokenizer as we will be using some of their features for preprocessing\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16680cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T16:24:47.963765Z",
     "iopub.status.busy": "2024-05-31T16:24:47.962855Z",
     "iopub.status.idle": "2024-05-31T16:24:47.977923Z",
     "shell.execute_reply": "2024-05-31T16:24:47.977094Z"
    },
    "id": "c16680cc",
    "papermill": {
     "duration": 0.054485,
     "end_time": "2024-05-31T16:24:47.979821",
     "exception": false,
     "start_time": "2024-05-31T16:24:47.925336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_training_data(train_ds):\n",
    "  question = [ q.strip() for q in train_ds['question']] # Cleanup questions - Removing extra spaces\n",
    "  context  = [ c.strip() for c in train_ds['context']]\n",
    "  pad_on_right = tokenizer.padding_side == 'right' # returns True if question is followed by context,\n",
    "                                                   # to support models where context is followed by question, - order of question and context will be swapped\n",
    "\n",
    "  # To handle very long contexts, split the context into multiple chunks with a sliding window between them\n",
    "  inputs = tokenizer(\n",
    "    question if pad_on_right else context,\n",
    "    context if pad_on_right else question,\n",
    "    max_length = 384,                 # setting max length of question + context to 100\n",
    "    stride = 128,                     # overlapping tokens between chunks\n",
    "    truncation = 'only_second' if pad_on_right else 'only_first',       # if question +  context is too long, truncate only context (2nd argument)\n",
    "    return_overflowing_tokens = True, # return chunks that have been truncated as well\n",
    "    return_offsets_mapping = True,    # return which chunk contains the answer\n",
    "    padding = 'max_length',\n",
    "\t)\n",
    "\n",
    "  # One context is broken into multiple chunks if it exceeds max_length.\n",
    "  # Creating a mapper that maps each context to its corresponding features (chunks)\n",
    "  sample_mapping = inputs.pop('overflow_to_sample_mapping')\n",
    "\n",
    "\n",
    "  # Create offset map to map start and end end indices of answer in context\n",
    "  offset_mapping = inputs.pop('offset_mapping')\n",
    "\n",
    "\n",
    "  # Now that context is split into chunks, we will now find the first and last token of the answer within the context\n",
    "  chunk_ans_start_pos = []\n",
    "  chunk_ans_end_pos = []\n",
    "\n",
    "  # Now that context is split into chunks, we will now find the first and last token of the answer within the context\n",
    "\n",
    "  chunk_ans_start_pos = []\n",
    "  chunk_ans_end_pos = []\n",
    "\n",
    "  for i, offset in enumerate(offset_mapping):\n",
    "\n",
    "\n",
    "    # Fetch input ids and CLS index\n",
    "    input_ids = inputs['input_ids'][i]\n",
    "    cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "    # calculate context start and end indices\n",
    "    # fetch sequence_ids to know what is the question and what is the context\n",
    "    seq_id = inputs.sequence_ids(i)\n",
    "\n",
    "    sample_idx = sample_mapping[i] # get index of current chunk\n",
    "    answer = train_ds['answers'][sample_idx] # get answers for each source\n",
    "\n",
    "    # if no answer is given, set cls_index as start and end char\n",
    "    if len(answer['answer_start']) == 0:\n",
    "      chunk_ans_start_pos.append(cls_index)\n",
    "      chunk_ans_end_pos.append(cls_index)\n",
    "    else:\n",
    "      # calculate ans start and end indices\n",
    "      ans_start_char = answer['answer_start'][0]\n",
    "      ans_end_char   = ans_start_char + len(answer['text'][0])\n",
    "\n",
    "\n",
    "      # get start and end char of current context\n",
    "      context_start_char = 0\n",
    "      while seq_id[context_start_char] != (1 if pad_on_right else 0): # 1 indicates start of context\n",
    "        context_start_char += 1\n",
    "\n",
    "      context_end_char = len(input_ids) - 1\n",
    "      while seq_id[context_end_char] != (1 if pad_on_right else 0): # continue reading 1's until 0 is encountered -> end of context\n",
    "        context_end_char -= 1\n",
    "\n",
    "\n",
    "      # for all the chunks, check if ans lies within context of that chunk\n",
    "      # if current context doesn't contain ans -> set cls_index\n",
    "      if not(\n",
    "          offset[context_start_char][0] <= ans_start_char and  offset[context_end_char][1] >= ans_end_char # checking if ans lies within current context\n",
    "      ):\n",
    "        chunk_ans_start_pos.append(cls_index)\n",
    "        chunk_ans_end_pos.append(cls_index)\n",
    "\n",
    "      else: # take start and end token positions\n",
    "        # from the start of the context, move along context tokens until you reach the ans start char.\n",
    "        # context_start_char should not go beyond total length if ans is the last word\n",
    "        while context_start_char < len(offset) and offset[context_start_char][0] <= ans_start_char:\n",
    "          context_start_char += 1\n",
    "        chunk_ans_start_pos.append(context_start_char - 1)\n",
    "\n",
    "        # move backwards from end of context until you reach the ans end char\n",
    "        while offset[context_end_char][1] >= ans_end_char:\n",
    "          context_end_char -= 1\n",
    "        chunk_ans_end_pos.append(context_end_char + 1)\n",
    "\n",
    "  # Add start and end positions to inputs\n",
    "  inputs['start_positions'] = chunk_ans_start_pos\n",
    "  inputs['end_positions'] = chunk_ans_end_pos\n",
    "\n",
    "  return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1827c",
   "metadata": {
    "id": "3ac1827c",
    "papermill": {
     "duration": 0.036341,
     "end_time": "2024-05-31T16:24:48.052658",
     "exception": false,
     "start_time": "2024-05-31T16:24:48.016317",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Apply the function to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a29d5",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "5b0913f8044d405ba186181935861b62",
      "6d6bc5fd437f4037917a0aadcc6ab655"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-05-31T16:24:48.128650Z",
     "iopub.status.busy": "2024-05-31T16:24:48.127970Z",
     "iopub.status.idle": "2024-05-31T16:25:47.657650Z",
     "shell.execute_reply": "2024-05-31T16:25:47.656389Z"
    },
    "id": "578a29d5",
    "outputId": "f5f0ab5f-1b2f-40c7-e814-76739bb02eab",
    "papermill": {
     "duration": 59.570652,
     "end_time": "2024-05-31T16:25:47.659880",
     "exception": false,
     "start_time": "2024-05-31T16:24:48.089228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0913f8044d405ba186181935861b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6bc5fd437f4037917a0aadcc6ab655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of records in original training data:  87599\n",
      "Number of records in processed training data:  88524\n",
      "\n",
      "\n",
      "Number of records in original validation data:  10570\n",
      "Number of records in processed training data:  10784\n"
     ]
    }
   ],
   "source": [
    "processed_train = train.map(\n",
    "    preprocess_training_data,\n",
    "    batched=True, # 1 row is being split into multiple chunks/ features\n",
    "    remove_columns = train.column_names,\n",
    ")\n",
    "\n",
    "processed_validation = validation.map(\n",
    "    preprocess_training_data,\n",
    "    batched=True, # 1 row is being split into multiple chunks/ features\n",
    "    remove_columns = train.column_names,\n",
    ")\n",
    "\n",
    "print('\\n')\n",
    "print('Number of records in original training data: ', len(train))\n",
    "print('Number of records in processed training data: ',len(processed_train))\n",
    "print('\\n')\n",
    "print('Number of records in original validation data: ',len(validation))\n",
    "print('Number of records in processed training data: ', len(processed_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b0b1db",
   "metadata": {
    "id": "d4b0b1db",
    "papermill": {
     "duration": 0.036556,
     "end_time": "2024-05-31T16:25:47.736401",
     "exception": false,
     "start_time": "2024-05-31T16:25:47.699845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## process_validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21174b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T16:25:47.811892Z",
     "iopub.status.busy": "2024-05-31T16:25:47.811102Z",
     "iopub.status.idle": "2024-05-31T16:25:47.820451Z",
     "shell.execute_reply": "2024-05-31T16:25:47.819681Z"
    },
    "id": "c21174b5",
    "papermill": {
     "duration": 0.049381,
     "end_time": "2024-05-31T16:25:47.822216",
     "exception": false,
     "start_time": "2024-05-31T16:25:47.772835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to fetch the text, we need to re-process the validation_ds and add:\n",
    "# 1. record_id to map each record to its corr features\n",
    "# 2. offset map - gives start and end chars of each token\n",
    "\n",
    "def process_validation_data(val_ds):\n",
    "  question = [ q.strip() for q in val_ds['question']] # Cleanup questions - Removing extra spaces\n",
    "  context  = [ c.strip() for c in val_ds['context']]\n",
    "  pad_on_right = tokenizer.padding_side == 'right'\n",
    "\n",
    "  # To handle very long contexts, split the context into multiple chunks with a sliding window between them\n",
    "  inputs = tokenizer(\n",
    "    question if pad_on_right else context,\n",
    "    context if pad_on_right else question,\n",
    "    max_length = 384, # setting max length of question + context to 384\n",
    "    stride = 128, # overlapping tokens between chunks\n",
    "    truncation = 'only_second' if pad_on_right else 'only_first',\n",
    "    return_overflowing_tokens = True,\n",
    "    return_offsets_mapping = True, # return start and end indices of answer in context\n",
    "    padding = 'max_length',  # padding to max length as contexts are long and no need for dynamic padding\n",
    "    )\n",
    "\n",
    "  # One context is broken into multiple chunks if it exceeds max_length.\n",
    "  # Creating a mapper that maps each context to its corresponding features (chunks)\n",
    "  sample_mapping = inputs.pop('overflow_to_sample_mapping')\n",
    "\n",
    "  # cleanup offset mapping: it contains offset for question + context\n",
    "  # set question offset to None so that we can identify Question and Context\n",
    "  inputs['record_id']=[]\n",
    "  for i in range(len(inputs['input_ids'])):\n",
    "    # Take seq ids to distinguish b/w q and c\n",
    "    seq_id = inputs.sequence_ids(i)\n",
    "    context_index = 1 if pad_on_right else 0\n",
    "\n",
    "    # Fetching current feature\n",
    "    sample_idx = sample_mapping[i]\n",
    "    # Add record_ids to inputs\n",
    "    inputs['record_id'].append(val_ds['id'][sample_idx]) # create record id list to tie created chunks back to their source\n",
    "\n",
    "\n",
    "    # update offset mapping to None for everything not part of context : seq_id is 1 for context and 0 for question\n",
    "    inputs['offset_mapping'][i] = [\n",
    "        (offset if seq_id[j] == context_index else None)\n",
    "        for j, offset in enumerate(inputs['offset_mapping'][i])\n",
    "    ]\n",
    "\n",
    "\n",
    "  return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c574986",
   "metadata": {
    "id": "5c574986",
    "papermill": {
     "duration": 0.036518,
     "end_time": "2024-05-31T16:25:47.895409",
     "exception": false,
     "start_time": "2024-05-31T16:25:47.858891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## postprocess_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c67aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T16:25:47.971398Z",
     "iopub.status.busy": "2024-05-31T16:25:47.971066Z",
     "iopub.status.idle": "2024-05-31T16:25:47.986383Z",
     "shell.execute_reply": "2024-05-31T16:25:47.985486Z"
    },
    "id": "34c67aaa",
    "papermill": {
     "duration": 0.055644,
     "end_time": "2024-05-31T16:25:47.988256",
     "exception": false,
     "start_time": "2024-05-31T16:25:47.932612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Track the progress of code execution with a progress bar\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "n_best_logits = 20\n",
    "max_ans_length = 30 # eliminate long ans\n",
    "\n",
    "def postprocess_predictions(examples, features, pred_start_logits, pred_end_logits):\n",
    "    # Mapping each example to its corresponding features\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    example_id_index_map = { k: i  for i, k in enumerate(examples['id']) }\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "         features_per_example[example_id_index_map[feature['record_id']]].append(i)\n",
    "\n",
    "    # Display number of examples and features\n",
    "    print(f'Post processing: {len(examples)} examples split into {len(features)} features')\n",
    "\n",
    "    # Create empty dict for storing predictions\n",
    "    predicted_ans = collections.OrderedDict()\n",
    "\n",
    "    # Looping over all examples\n",
    "    for i, example in enumerate(tqdm(examples)):\n",
    "        # fetch features associated to current example\n",
    "        features_indices = features_per_example[i]\n",
    "        context = example['context']\n",
    "        answers = []\n",
    "        # For squadv2 - handling impossible answers\n",
    "        min_null_score = None\n",
    "\n",
    "        # Looping over all features associated to current example\n",
    "        for feature_index in features_indices:\n",
    "            # get the model predictions and offsets for this feature\n",
    "            start_logits = pred_start_logits[feature_index]\n",
    "            end_logits   = pred_end_logits[feature_index]\n",
    "            offsets      = features[feature_index]['offset_mapping']\n",
    "\n",
    "            # squad_v2 Impossible answers: update minimum null prediction\n",
    "            # for impossible ans, start and end index = CLS token index\n",
    "            cls_index = features[feature_index]['input_ids'].index(tokenizer.cls_token_id)\n",
    "            # compute the score for impossible ans for this feature:\n",
    "            # 1. ans is simply not in the current feature => not an impossible ans\n",
    "            # 2. it is truly an impossible ans\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            # get the best impossible ans score for an example\n",
    "            # update the null score if impossible ans score for this feature > prev impossible ans score\n",
    "            if min_null_score is None or feature_null_score > min_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "\n",
    "            # list of n_best indexes\n",
    "            n_best_size=20\n",
    "            max_answer_length=30\n",
    "            start_indexes = np.argsort(start_logits)[-1: -n_best_logits-1: -1].tolist()\n",
    "            end_indexes   = np.argsort(end_logits)[-1: -n_best_logits-1: -1].tolist()\n",
    "\n",
    "            for start_idx in start_indexes:\n",
    "                for end_idx in end_indexes:\n",
    "                    # check if indices are within bounds or ans is fully in the context else skip\n",
    "                    if  (start_idx >= len(offsets) or end_idx >= len(offsets) or\n",
    "                    offsets[start_idx] is None or offsets[end_idx] is None):\n",
    "                        continue\n",
    "                    # check if ans length is not < 0 or > maxlength then skip\n",
    "                    if end_idx < start_idx or end_idx - start_idx + 1 > max_ans_length:\n",
    "                        continue\n",
    "                    start_char = offsets[start_idx][0]\n",
    "                    end_char   = offsets[end_idx][1]\n",
    "                    answers.append(\n",
    "                    {\n",
    "                        'text' : context[start_char : end_char],\n",
    "                        'score': start_logits[start_idx] + end_logits[end_idx]\n",
    "                    }\n",
    "                    )\n",
    "\n",
    "        if len(answers)>0:\n",
    "            best_ans = max(answers, key = lambda x: x['score']) #selecting ans with highest logit score as best\n",
    "        else:\n",
    "            best_ans = {'text': '', 'score': 0.0}\n",
    "\n",
    "        # calculate the final ans: best ans or null ans (for squad_v2)\n",
    "        if not squad_v2:\n",
    "            predicted_ans[example['id']] = best_ans['text']\n",
    "        else: # if normal ans score > impossible ans score take normal ans else blank\n",
    "            final_ans = best_ans['text'] if best_ans['score'] > min_null_score else ''\n",
    "            predicted_ans[example['id']] = final_ans\n",
    "    return predicted_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb8f30e",
   "metadata": {
    "id": "efb8f30e",
    "papermill": {
     "duration": 0.036403,
     "end_time": "2024-05-31T16:25:48.061656",
     "exception": false,
     "start_time": "2024-05-31T16:25:48.025253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Fine-tuning the model\n",
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d178ebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T16:25:48.137369Z",
     "iopub.status.busy": "2024-05-31T16:25:48.136700Z",
     "iopub.status.idle": "2024-05-31T16:25:48.141866Z",
     "shell.execute_reply": "2024-05-31T16:25:48.141025Z"
    },
    "id": "3d178ebc",
    "papermill": {
     "duration": 0.044845,
     "end_time": "2024-05-31T16:25:48.143672",
     "exception": false,
     "start_time": "2024-05-31T16:25:48.098827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up training parameters\n",
    "model_name = model_checkpoint.split('/')[-1]\n",
    "huggingface_model_name = f'{model_name}-finetuned-squad-v2'\n",
    "\n",
    "# set mlflow params\n",
    "os.environ['MLFLOW_EXPERIMENT_NAME'] = 'Distilbert_model'\n",
    "os.environ['MLFLOW_FLATTEN_PARAMS'] = '1'\n",
    "\n",
    "# parameters obtained using hyperopt\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 2\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b24b2",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b3c5c3567f0f484fa878fd7bfe896dc1",
      "9b3aaf0000434546a1be7c2dc3717dd9",
      "f47d14e397ae45f1a9949d9eaf9cd759",
      "9b786f863d5f4e4a9508ab3d7d42ac82",
      "63ddbed1cc754501a57ead6a7d3b0203",
      "d6e2f184282e4a2c88f27ec47985ee7a"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-05-31T16:25:48.219637Z",
     "iopub.status.busy": "2024-05-31T16:25:48.218877Z",
     "iopub.status.idle": "2024-05-31T16:27:05.383878Z",
     "shell.execute_reply": "2024-05-31T16:27:05.383095Z"
    },
    "id": "be4b24b2",
    "outputId": "e70440ab-fe8b-4751-9e75-315525e0cb64",
    "papermill": {
     "duration": 77.204844,
     "end_time": "2024-05-31T16:27:05.385966",
     "exception": false,
     "start_time": "2024-05-31T16:25:48.181122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/Mariah64/distilbert-base-uncased-finetuned-squad-v2 into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c5c3567f0f484fa878fd7bfe896dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file tf_model.h5:   0%|          | 8.00k/253M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3aaf0000434546a1be7c2dc3717dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file logs/validation/events.out.tfevents.1717140053.f29979c1d7a6.24.1.v2: 100%|##########| 900/900 [0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47d14e397ae45f1a9949d9eaf9cd759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file logs/train/events.out.tfevents.1717137962.f29979c1d7a6.24.0.v2:   2%|1         | 24.0k/1.37M [00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b786f863d5f4e4a9508ab3d7d42ac82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file logs/validation/events.out.tfevents.1717140053.f29979c1d7a6.24.1.v2: 100%|##########| 900/900 [00:0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ddbed1cc754501a57ead6a7d3b0203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file logs/train/events.out.tfevents.1717137962.f29979c1d7a6.24.0.v2:   0%|          | 1.00k/1.37M [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e2f184282e4a2c88f27ec47985ee7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file tf_model.h5:   0%|          | 1.00k/253M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define callbacks\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"./qa_model_save\",\n",
    "    tokenizer=tokenizer,\n",
    "    hub_model_id=huggingface_model_name,\n",
    ")\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=\"./qa_model_save/logs\", histogram_freq=1)\n",
    "\n",
    "callbacks = [push_to_hub_callback ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d4ee5",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "def489039c0c4f0b96167387e89a7d0d",
      "d0edd512f05b4633a32f7bbe6027bdd1"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-05-31T16:27:05.464142Z",
     "iopub.status.busy": "2024-05-31T16:27:05.463381Z",
     "iopub.status.idle": "2024-05-31T16:27:18.602376Z",
     "shell.execute_reply": "2024-05-31T16:27:18.601518Z"
    },
    "id": "005d4ee5",
    "outputId": "fb3abddf-4ec9-4a2e-a10d-7f301c2039c6",
    "papermill": {
     "duration": 13.180542,
     "end_time": "2024-05-31T16:27:18.604813",
     "exception": false,
     "start_time": "2024-05-31T16:27:05.424271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def489039c0c4f0b96167387e89a7d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForQuestionAnswering: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0edd512f05b4633a32f7bbe6027bdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint) # Using from_pretrained to download and cache the model\n",
    "\n",
    "# convert dataset to tf datasets\n",
    "train_ds = model.prepare_tf_dataset(\n",
    "    processed_train,\n",
    "    shuffle = True,\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "validation_ds = model.prepare_tf_dataset(\n",
    "    processed_validation,\n",
    "    shuffle = False,\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "# Reprocess the validation data\n",
    "reprocessed_validation = validation.map(\n",
    "    process_validation_data,\n",
    "    batched = True,\n",
    "    remove_columns = validation.column_names,\n",
    ")\n",
    "\n",
    "# Convert it to tf dataset\n",
    "reprocessed_validation_tf = model.prepare_tf_dataset(\n",
    "    reprocessed_validation,\n",
    "    shuffle = False,\n",
    "    batch_size = batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "# create optimizer from trnasformers uses AdamW optimizer with weight decay\n",
    "from transformers import create_optimizer\n",
    "optimizer, _ = create_optimizer(\n",
    "    init_lr = learning_rate,\n",
    "    num_train_steps = len(train_ds) * num_epochs,\n",
    "    num_warmup_steps = 0\n",
    ")\n",
    "\n",
    "# compile the model, no need to mention loss as model automatically handles it\n",
    "model.compile(optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf0e7ec",
   "metadata": {
    "id": "9cf0e7ec",
    "papermill": {
     "duration": 0.037968,
     "end_time": "2024-05-31T16:27:18.682825",
     "exception": false,
     "start_time": "2024-05-31T16:27:18.644857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f352e87",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "266b586c118b431280e72359a8ace9e8",
      "cd4e8ea6c85e4784bcba189b59f378c5",
      "3b8423eebf8548d3841202abcbb8a9b5"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-05-31T16:27:18.760772Z",
     "iopub.status.busy": "2024-05-31T16:27:18.760418Z",
     "iopub.status.idle": "2024-05-31T16:30:18.725414Z",
     "shell.execute_reply": "2024-05-31T16:30:18.724581Z"
    },
    "id": "0f352e87",
    "outputId": "49efd4ea-d255-4ea5-82f4-af2e53134bbe",
    "papermill": {
     "duration": 180.006562,
     "end_time": "2024-05-31T16:30:18.727710",
     "exception": false,
     "start_time": "2024-05-31T16:27:18.721148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 99s 243ms/step\n",
      "Post processing: 10570 examples split into 10784 features\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266b586c118b431280e72359a8ace9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4e8ea6c85e4784bcba189b59f378c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8423eebf8548d3841202abcbb8a9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    # Loading the pre-trained weights\n",
    "    checkpoint_path = '/kaggle/working/qa_model_save/tf_model.h5'\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "\n",
    "    # Make predictions\n",
    "    pred_val = model.predict(reprocessed_validation_tf)\n",
    "\n",
    "\n",
    "    # Post-processing predictions -  FINAL PREDICTIONS\n",
    "    final_predictions = postprocess_predictions(\n",
    "        validation,\n",
    "        reprocessed_validation,\n",
    "        pred_val['start_logits'],\n",
    "        pred_val['end_logits'],\n",
    "    )\n",
    "\n",
    "\n",
    "    # EVALUATION\n",
    "\n",
    "    # Load the squad evaluation metric\n",
    "    metric = load_metric('squad_v2' if squad_v2 else 'squad')\n",
    "\n",
    "    # Formatting the final predictiosn to a list of dict as expected by the squad evaluation metric we will use.\n",
    "    if squad_v2:\n",
    "        formatted_predictions = [\n",
    "            {'id': key, 'prediction_text': value, 'no_answer_probability':0.0} # no_ans_prob=0 since we explicity set it to blank for no ans\n",
    "            for key, value in final_predictions.items()\n",
    "        ]\n",
    "    else:\n",
    "        formatted_predictions = [\n",
    "            {'id': key, 'prediction_text': value}\n",
    "            for key, value in final_predictions.items()\n",
    "        ]\n",
    "\n",
    "    actual_ans = [\n",
    "        {'id': example['id'], 'answers': example['answers']}\n",
    "        for example in validation\n",
    "    ]\n",
    "\n",
    "    # Evalute using the metric\n",
    "    eval_metrics = metric.compute(predictions = formatted_predictions, references = actual_ans)\n",
    "    exact_match = eval_metrics['exact_match']\n",
    "    f1_score = eval_metrics['f1']\n",
    "\n",
    "    params = {\n",
    "    'learning_rate' : learning_rate,\n",
    "    'num_epochs' : num_epochs,\n",
    "    'batch_size' : batch_size\n",
    "    }\n",
    "\n",
    "    metrics = {\n",
    "        'exact_match': exact_match,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "\n",
    "    # log params\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # log eval metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42422952",
   "metadata": {
    "id": "42422952",
    "papermill": {
     "duration": 0.064216,
     "end_time": "2024-05-31T16:30:18.857438",
     "exception": false,
     "start_time": "2024-05-31T16:30:18.793222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Copy mlruns folder to main folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c7e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T16:30:18.990206Z",
     "iopub.status.busy": "2024-05-31T16:30:18.989477Z",
     "iopub.status.idle": "2024-05-31T16:30:19.005444Z",
     "shell.execute_reply": "2024-05-31T16:30:19.004700Z"
    },
    "id": "9c0c7e9a",
    "papermill": {
     "duration": 0.083412,
     "end_time": "2024-05-31T16:30:19.007333",
     "exception": false,
     "start_time": "2024-05-31T16:30:18.923921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "cp -r mlruns qa_model_save\n",
    "cd qa_model_save\n",
    "git add mlruns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411b203",
   "metadata": {
    "id": "b411b203",
    "papermill": {
     "duration": 0.064338,
     "end_time": "2024-05-31T16:30:19.136484",
     "exception": false,
     "start_time": "2024-05-31T16:30:19.072146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Upload mlruns to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df2179b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T16:30:19.268881Z",
     "iopub.status.busy": "2024-05-31T16:30:19.267762Z",
     "iopub.status.idle": "2024-05-31T16:30:22.040507Z",
     "shell.execute_reply": "2024-05-31T16:30:22.039316Z"
    },
    "id": "0df2179b",
    "outputId": "acf60695-9081-448b-8522-2dc01986e99d",
    "papermill": {
     "duration": 2.841384,
     "end_time": "2024-05-31T16:30:22.042691",
     "exception": false,
     "start_time": "2024-05-31T16:30:19.201307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/mlruns/ (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/0/ (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/0/meta.yaml (deflated 25%)\r\n",
      "  adding: kaggle/working/mlruns/.trash/ (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/ (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/ (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/params/ (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/params/batch_size (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/params/num_epochs (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/params/learning_rate (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/tags/ (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/tags/mlflow.source.type (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/tags/mlflow.user (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/tags/mlflow.source.name (deflated 3%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/tags/mlflow.runName (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/meta.yaml (deflated 44%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/artifacts/ (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/metrics/ (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/metrics/f1_score (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/df52c141ae3642dab2f11c1655ef6914/metrics/exact_match (stored 0%)\r\n",
      "  adding: kaggle/working/mlruns/567976060329198296/meta.yaml (deflated 30%)\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='mlruns.zip' target='_blank'>mlruns.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/mlruns.zip"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=\"qa_model_save\",\n",
    "    repo_id=\"Mariah64/distilbert-base-uncased-finetuned-squad-v2\",\n",
    "#     repo_type=\"space\",\n",
    ")\n",
    "\n",
    "\n",
    "# download mlruns zip to view in mlflow ui\n",
    "!zip -r mlruns.zip /kaggle/working/mlruns\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'mlruns.zip')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 44767,
     "sourceId": 53352,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30715,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 395.438096,
   "end_time": "2024-05-31T16:30:25.460536",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-31T16:23:50.022440",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
